{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSR: Automatic Stuttered Speech Recoginition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !conda install tensorflow\n",
    "# !conda activate tensorflow\n",
    "# !pip install --upgrade librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "#%matplotlib inline\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import shutil\n",
    "import datetime\n",
    "import logging\n",
    "import colorlog\n",
    "import progressbar\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setting up progressbar and logger\n",
    "# progressbar.streams.wrap_stderr()\n",
    "logger = colorlog.getLogger(\"ASSR\")\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(colorlog.ColoredFormatter('%(log_color)s%(levelname)-8s| %(message)s'))\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction:\n",
    "    def __init__(self, n_mels=128):\n",
    "        self.n_mels = n_mels\n",
    "        self.y = None\n",
    "        self.sr = None\n",
    "        self.S = None\n",
    "        self.log_S = None\n",
    "        self.mfcc = None\n",
    "        self.delta_mfcc = None\n",
    "        self.delta2_mfcc = None\n",
    "        self.M = None\n",
    "        self.rms = None\n",
    "    \n",
    "    def loadFile(self, filename):\n",
    "        self.y, self.sr = librosa.load(filename)\n",
    "        logger.debug('File loaded: %s', filename)\n",
    "    \n",
    "    def load_y_sr(self, y, sr):\n",
    "        self.y = y\n",
    "        self.sr = sr\n",
    "    \n",
    "    def melspectrogram(self):\n",
    "        self.S = librosa.feature.melspectrogram(self.y, sr=self.sr, n_mels=self.n_mels)\n",
    "        self.log_S = librosa.amplitude_to_db(self.S, ref=np.max)\n",
    "    \n",
    "    def plotmelspectrogram(self):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        librosa.display.specshow(self.log_S, sr=self.sr, x_axis='time', y_axis='mel')\n",
    "        plt.title('mel Power Spectrogram')\n",
    "        plt.colorbar(format='%+02.0f dB')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    def extractmfcc(self, n_mfcc=13):\n",
    "        self.mfcc = librosa.feature.mfcc(S=self.log_S, n_mfcc=n_mfcc)\n",
    "#         self.delta_mfcc = librosa.feature.delta(self.mfcc)\n",
    "# HERE IT IS\n",
    "        self.delta_mfcc = librosa.feature.delta(self.mfcc,mode='nearest')\n",
    "        self.delta2_mfcc = librosa.feature.delta(self.mfcc, order=2,mode='nearest')\n",
    "        self.M = np.vstack([self.mfcc, self.delta_mfcc, self.delta2_mfcc])\n",
    "    \n",
    "    def plotmfcc(self):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(3, 1, 1)\n",
    "        librosa.display.specshow(self.mfcc)\n",
    "        plt.ylabel('MFCC')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.subplot(3, 1, 2)\n",
    "        librosa.display.specshow(self.delta_mfcc)\n",
    "        plt.ylabel('MFCC-$\\Delta$')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.subplot(3, 1, 3)\n",
    "        librosa.display.specshow(self.delta2_mfcc, sr=self.sr, x_axis='time')\n",
    "        plt.ylabel('MFCC-$\\Delta^2$')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "    \n",
    "    def extractrms(self):\n",
    "        self.rms = librosa.feature.rms(y=self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, datasetDir, datasetLabelFilename, datasetArrayFilename):\n",
    "        self.n_features = 80\n",
    "        logger.info(\"Number of features: %s\", self.n_features)\n",
    "        self.X = np.empty(shape=(0, self.n_features))\n",
    "        self.Y = np.empty(shape=(0, 2))\n",
    "        \n",
    "        self.datasetArrayFilename = datasetArrayFilename\n",
    "        logger.debug(\"Dataset array filename: %s\", self.datasetArrayFilename)\n",
    "        \n",
    "        if os.path.isfile(self.datasetArrayFilename):    \n",
    "            self.__readFromFile()\n",
    "        else:\n",
    "            self.datasetDir = datasetDir\n",
    "            logger.debug(\"Dataset Directory: %s\", self.datasetDir)\n",
    "\n",
    "            self.datasetLabelFilename = datasetLabelFilename\n",
    "            logger.debug(\"Dataset labels filename: %s\", self.datasetLabelFilename)\n",
    "\n",
    "            if not os.path.isdir(self.datasetDir) or not os.path.isfile(self.datasetLabelFilename):\n",
    "                logger.info(\"%s or %s does not exists\", self.datasetDir, self.datasetLabelFilename)\n",
    "                self.__buildDatasetAndLabels('wav/release1')\n",
    "                \n",
    "            self.__build()\n",
    "            self.__writeToFile()\n",
    "    \n",
    "    def __build(self):\n",
    "        logger.info(\"Building dataset from directory: %s\", self.datasetDir)\n",
    "        num_lines = sum(1 for line in open(self.datasetLabelFilename, 'r'))\n",
    "        with open(self.datasetLabelFilename, 'r') as datasetLabelFile:\n",
    "            filesProcessed=0\n",
    "            pbar = progressbar.ProgressBar(redirect_stdout=True)\n",
    "            for line in pbar(datasetLabelFile, max_value=num_lines):\n",
    "                lineSplit = line.strip().split(' ')\n",
    "                audiofilename = lineSplit[0]\n",
    "                label = lineSplit[1]\n",
    "                try:\n",
    "                    features = FeatureExtraction()\n",
    "                    features.loadFile(os.path.join(self.datasetDir, audiofilename))\n",
    "                    features.melspectrogram()\n",
    "                    features.extractmfcc()\n",
    "# HERE IT IS\n",
    "#                     features.extractmfcc(mode='nearest')\n",
    "                    features.extractrms()\n",
    "                except ValueError:\n",
    "                    logger.warning(\"Error in extracting features from file %s\", audiofilename)\n",
    "                    continue\n",
    "                \n",
    "                featureVector = []\n",
    "                for feature in features.mfcc:\n",
    "                    featureVector.append(np.mean(feature))\n",
    "                    featureVector.append(np.var(feature))\n",
    "                \n",
    "                for feature in features.delta_mfcc:\n",
    "                    featureVector.append(np.mean(feature))\n",
    "                    featureVector.append(np.var(feature))\n",
    "                \n",
    "                for feature in features.delta2_mfcc:\n",
    "                    featureVector.append(np.mean(feature))\n",
    "                    featureVector.append(np.var(feature))\n",
    "                \n",
    "                featureVector.append(np.mean(features.rms))\n",
    "                featureVector.append(np.var(features.rms))\n",
    "                \n",
    "                self.X = np.vstack((self.X, [featureVector]))\n",
    "                \n",
    "                if label == \"STUTTER\":\n",
    "                    self.Y = np.vstack((self.Y, [0, 1]))\n",
    "                elif label == \"NORMAL\":\n",
    "                    self.Y = np.vstack((self.Y, [1, 0]))\n",
    "                else:\n",
    "                    logger.error(\"Unexpected label: %s\", label)\n",
    "                    sys.exit()\n",
    "                \n",
    "                filesProcessed += 1            \n",
    "            \n",
    "            logger.info(\"Total files processed: %d\", filesProcessed)\n",
    "    \n",
    "    def __buildDatasetAndLabels(self, audioAndChaFilesDirectory):\n",
    "        logger.info(\"Rebuilding the dataset directory and labels\")\n",
    "        if os.path.isdir(self.datasetDir):\n",
    "            shutil.rmtree(self.datasetDir)\n",
    "        os.makedirs(self.datasetDir)\n",
    "        \n",
    "        labelFile = open(self.datasetLabelFilename, 'w')\n",
    "        \n",
    "        splitDuration = 300 # milliseconds\n",
    "        pbar = progressbar.ProgressBar(redirect_stdout=True)\n",
    "        for chaFileName in pbar(os.listdir(audioAndChaFilesDirectory)):\n",
    "            if chaFileName.endswith(\".cha\"):\n",
    "                subject = chaFileName.split('.')[0]\n",
    "                wavFileName = subject + \".wav\"\n",
    "                y, sr = librosa.load(os.path.join(audioAndChaFilesDirectory, wavFileName))\n",
    "\n",
    "                logger.debug(\"Parsing file: %s\", chaFileName)\n",
    "\n",
    "                with open(os.path.join(audioAndChaFilesDirectory, chaFileName), 'r') as chaFile:\n",
    "                    sndFound = False\n",
    "                    phoFound = False\n",
    "                    startTime = -1\n",
    "                    endTime = -1\n",
    "                    label = None\n",
    "                    for line in chaFile:\n",
    "                        if not sndFound:\n",
    "                            if re.search(r\"%snd:\", line):\n",
    "                                lineSplit = line.split(\"_\")\n",
    "                                startTime = int(lineSplit[-2])\n",
    "                                endTime = lineSplit[-1]\n",
    "                                endTime = int(re.sub(r\"\\u0015\\n\", '', endTime))\n",
    "                                sndFound = True\n",
    "                        else:\n",
    "                            if re.search(r\"%pho:\", line):\n",
    "                                if re.search(r'[A-Z]', line):\n",
    "                                    label = \"STUTTER\"\n",
    "                                else:\n",
    "                                    label = \"NORMAL\"\n",
    "                                phoFound = True\n",
    "                        if sndFound and phoFound:\n",
    "                            n_splits = int(np.round((endTime - startTime) / splitDuration))\n",
    "                            \n",
    "                            startingSample = int(startTime * sr / 1000)\n",
    "                            for i in range(1, n_splits):\n",
    "                                endingSample = int(startingSample + (splitDuration * sr / 1000))\n",
    "                                audiofilename = subject + \":\" + str(startTime) + \":\" + str(int(startTime) + splitDuration) + \".wav\"\n",
    "                                labelFile.write(audiofilename + \" \" + label + \"\\n\")\n",
    "                                audio = y[startingSample:endingSample]\n",
    "                                librosa.output.write_wav(os.path.join(self.datasetDir, audiofilename), audio, sr)\n",
    "                                \n",
    "                                startingSample = endingSample\n",
    "                                startTime = int(startTime) + splitDuration\n",
    "                            \n",
    "                            endingSample = int(endTime * sr / 1000)\n",
    "                            audiofilename = subject + \":\" + str(startTime) + \":\" + str(endTime) + \".wav\"\n",
    "                            labelFile.write(audiofilename + \" \" + label + \"\\n\")\n",
    "                            audio = y[startingSample:endingSample]\n",
    "                            librosa.output.write_wav(os.path.join(self.datasetDir, audiofilename), audio, sr)\n",
    "                            \n",
    "                            \n",
    "                            sndFound = False\n",
    "                            phoFound = False\n",
    "                            startTime = -1\n",
    "                            endTime = -1\n",
    "                            label = None\n",
    "\n",
    "        labelFile.close()\n",
    "    \n",
    "    def __writeToFile(self, filename=None):\n",
    "        if filename == None:\n",
    "            filename = self.datasetArrayFilename\n",
    "            \n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        np.savetxt(filename, np.hstack((self.X, self.Y)))\n",
    "        logger.info(\"Array stored in file %s\", filename)\n",
    "    \n",
    "    def __readFromFile(self, filename=None):\n",
    "        if filename == None:\n",
    "            filename = self.datasetArrayFilename\n",
    "            \n",
    "        if not os.path.isfile(filename):\n",
    "            logger.error(\"%s does not exists or is not a file\", filename)\n",
    "            sys.exit()\n",
    "        matrix = np.loadtxt(filename)\n",
    "        self.X = matrix[:, 0:self.n_features]\n",
    "        self.Y = matrix[:, self.n_features:]\n",
    "        logger.info(\"Array read from file %s\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, X_train=None, Y_train=None, X_test=None, Y_test=None):\n",
    "        # Data\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        \n",
    "        # Learning Parameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.training_epochs = 1200\n",
    "        self.batch_size = 100\n",
    "        self.display_step = 100\n",
    "\n",
    "        # Model Parameters\n",
    "        self.n_hidden = [10, 10, 10]\n",
    "        self.hiddenLayers = len(self.n_hidden)\n",
    "        self.n_input = 80\n",
    "        self.n_classes = 2\n",
    "\n",
    "        logger.debug(\"Neural network of depth %d\", self.hiddenLayers)\n",
    "        for i in range(self.hiddenLayers):\n",
    "            logger.debug(\"Depth of layer %d is %d\", (i + 1), self.n_hidden[i])\n",
    "\n",
    "        self.x = tf.placeholder(\"float\", [None, self.n_input])\n",
    "        self.y = tf.placeholder(\"float\", [None, self.n_classes])\n",
    "        self.layer = None\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        # Model\n",
    "        self.model = self.__network(self.x)\n",
    "        self.save_path = None\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.model, labels=self.y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        # Initialize the variables\n",
    "        self.init = tf.global_variables_initializer()\n",
    "    \n",
    "    def setTrainData(self, X, Y):\n",
    "        self.X_train = X\n",
    "        self.Y_train = Y\n",
    "        \n",
    "    def setTestData(self, X, Y):\n",
    "        self.X_test = X\n",
    "        self.Y_test = Y\n",
    "        \n",
    "    def __network(self, x):\n",
    "        self.layer = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for n_layer in range(self.hiddenLayers):\n",
    "            if n_layer == 0:\n",
    "                self.weights.append(tf.Variable(tf.random_normal([self.n_input, self.n_hidden[n_layer]])))\n",
    "                self.biases.append(tf.Variable(tf.random_normal([self.n_hidden[n_layer]])))\n",
    "                self.layer.append(tf.nn.relu(tf.add(tf.matmul(x, self.weights[n_layer]), self.biases[n_layer])))\n",
    "            else:\n",
    "                self.weights.append(tf.Variable(tf.random_normal([self.n_hidden[n_layer - 1], self.n_hidden[n_layer]])))\n",
    "                self.biases.append(tf.Variable(tf.random_normal([self.n_hidden[n_layer]])))\n",
    "                self.layer.append(tf.nn.relu(tf.add(tf.matmul(self.layer[n_layer - 1], self.weights[n_layer]), self.biases[n_layer])))\n",
    "\n",
    "\n",
    "        # Output layer\n",
    "        self.weights.append(tf.Variable(tf.random_normal([self.n_hidden[self.hiddenLayers - 1], self.n_classes])))\n",
    "        self.biases.append(tf.Variable(tf.random_normal([self.n_classes])))\n",
    "        self.layer.append(tf.matmul(self.layer[self.hiddenLayers - 1], self.weights[self.hiddenLayers]) + self.biases[self.hiddenLayers])\n",
    "\n",
    "        return self.layer[self.hiddenLayers]\n",
    "    \n",
    "    def train(self):\n",
    "        logger.info(\"Training the neural network\")\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(self.init)\n",
    "            pbarWidgets = [\n",
    "                progressbar.Percentage(),\n",
    "                ' (',\n",
    "                progressbar.SimpleProgress(),\n",
    "                ') ',\n",
    "                progressbar.Bar(),\n",
    "                ' ',\n",
    "                progressbar.Timer(),\n",
    "                ' ',\n",
    "                progressbar.ETA(),\n",
    "                ' ',\n",
    "                progressbar.DynamicMessage('Cost'),\n",
    "            ]\n",
    "            with progressbar.ProgressBar(max_value=self.training_epochs, redirect_stdout=True, widgets=pbarWidgets) as pbar:\n",
    "                for epoch in range(self.training_epochs):\n",
    "                    avg_cost = 0\n",
    "                    total_batch = int(len(self.X_train) / self.batch_size)\n",
    "                    X_batches = np.array_split(self.X_train, total_batch)\n",
    "                    Y_batches = np.array_split(self.Y_train, total_batch)\n",
    "\n",
    "                    for i in range(total_batch):\n",
    "                        batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "                        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                        _, c = sess.run([self.optimizer, self.cost], feed_dict={self.x: batch_x, self.y: batch_y})\n",
    "\n",
    "                        # Compute average loss\n",
    "                        avg_cost += c / total_batch\n",
    "\n",
    "                    pbar.update(epoch + 1, Cost=avg_cost)\n",
    "                \n",
    "            logger.info(\"Optimization Finished!\")\n",
    "\n",
    "            evalAccuracy = self.__getAccuracy()\n",
    "            \n",
    "            global result \n",
    "            result = tf.argmax(self.model, 1).eval({self.x: self.X_test, self.y: self.Y_test})\n",
    "            \n",
    "            tfSessionsDir = \"tfSessions\"\n",
    "            if not os.path.isdir(tfSessionsDir):\n",
    "                os.makedirs(tfSessionsDir)\n",
    "            timestamp = '{:%Y-%m-%d-%H:%M:%S}'.format(datetime.datetime.now()) + '-' + str(evalAccuracy)\n",
    "            os.makedirs(os.path.join(tfSessionsDir, timestamp))\n",
    "            modelfilename =  os.path.join(os.path.join(tfSessionsDir, timestamp), 'session.ckpt')\n",
    "            self.save_path = saver.save(sess, modelfilename)\n",
    "            \n",
    "            with open(os.path.join(os.path.join(tfSessionsDir, timestamp), 'details.txt'), 'w') as details:\n",
    "                details.write(\"learning_rate = \" + str(self.learning_rate) + \"\\n\")\n",
    "                details.write(\"training_epochs = \" + str(self.training_epochs) + \"\\n\")\n",
    "                details.write(\"batch_size = \" + str(self.batch_size) + \"\\n\")\n",
    "                details.write(\"display_step = \" + str(self.display_step) + \"\\n\")\n",
    "                details.write(\"n_hidden = \" + str(self.n_hidden) + \"\\n\")\n",
    "                details.write(\"hiddenLayers = \" + str(self.hiddenLayers) + \"\\n\")\n",
    "                details.write(\"n_input = \" + str(self.n_input) + \"\\n\")\n",
    "                details.write(\"n_classes = \" + str(self.n_classes) + \"\\n\")\n",
    "                \n",
    "            logger.info(\"Model saved in file: %s\" % self.save_path)\n",
    "    \n",
    "    def getModelPath(self):\n",
    "        return self.save_path\n",
    "        \n",
    "    def __getAccuracy(self):\n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(self.model, 1), tf.argmax(self.y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        evalAccuracy = accuracy.eval({self.x: self.X_test, self.y: self.Y_test})\n",
    "        logger.info(\"Accuracy: %f\", evalAccuracy)\n",
    "        return evalAccuracy\n",
    "        \n",
    "    def loadAndClassify(self, filename, X):            \n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, filename)\n",
    "            prediction_model = tf.argmax(self.model, 1)\n",
    "            return prediction_model.eval({self.x: X})\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using the NN model for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCorrection():\n",
    "    def __init__(self, audiofile, tfSessionFile, segmentLength=300, segmentHop=100, n_features=80, correctionsDir='corrections'):\n",
    "        self.tfSessionFile = tfSessionFile\n",
    "        self.segmentLength = segmentLength\n",
    "        self.segmentHop = segmentHop\n",
    "        self.n_features = n_features\n",
    "        self.correctionsDir = correctionsDir\n",
    "        self.samplesPerSegment = None\n",
    "        self.samplesToSkipPerHop = None\n",
    "        self.upperLimit = None\n",
    "        self.inputFilename = None\n",
    "        self.y = None\n",
    "        self.sr = None\n",
    "        self.target_sr = 16000\n",
    "        NORMAL = 0\n",
    "        STUTTER = 1\n",
    "        self.speech = {NORMAL: [], STUTTER: []}\n",
    "        self.smoothingSamples = 1000\n",
    "        self.__loadfile(audiofile)\n",
    "    \n",
    "    def __loadfile(self, inputFilename):\n",
    "        if not os.path.isfile(inputFilename):\n",
    "            logger.error(\"%s does not exists or is not a file\", inputFilename)\n",
    "            sys.exit()\n",
    "        self.inputFilename = inputFilename\n",
    "        logger.info(\"Loading file %s\", self.inputFilename)\n",
    "        self.y, self.sr = librosa.load(self.inputFilename)\n",
    "        self.samplesPerSegment = int(self.segmentLength * self.sr / 1000)\n",
    "        self.samplesToSkipPerHop = int(self.segmentHop * self.sr / 1000)\n",
    "        self.upperLimit = len(self.y) - self.samplesPerSegment\n",
    "\n",
    "    def process(self):\n",
    "        logger.info(\"Attempting to correct %s\", self.inputFilename)\n",
    "        X = np.empty(shape=(0, self.n_features))\n",
    "        durations = np.empty(shape=(0, 2))\n",
    "\n",
    "        pbar = progressbar.ProgressBar()\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for start in pbar(range(0, self.upperLimit, self.samplesToSkipPerHop)):\n",
    "            end = start + self.samplesPerSegment\n",
    "            audio = self.y[start:end]\n",
    "\n",
    "            featureVector = self.__getFeatureVector(audio, self.sr)\n",
    "            if featureVector != None:\n",
    "                X = np.vstack((X, [featureVector]))\n",
    "                durations = np.vstack((durations, [start, end]))\n",
    "        \n",
    "        audio = self.y[end:]\n",
    "        featureVector = self.__getFeatureVector(audio, self.sr)\n",
    "        if featureVector != None:\n",
    "            X = np.vstack((X, [featureVector]))\n",
    "            durations = np.vstack((durations, [end, self.upperLimit + self.samplesPerSegment]))\n",
    "        logger.debug(\"Finished extracting features\")\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        nn = NeuralNetwork()\n",
    "        classificationResult = nn.loadAndClassify(self.tfSessionFile, X)\n",
    "        logger.debug(\"Finished classification of segments\")\n",
    "        \n",
    "        currentSegment = {'type': classificationResult[0], 'start': durations[0][0], 'end': durations[0][1]}\n",
    "        for (label, [start, end]) in zip(classificationResult[1:], durations[1:]):\n",
    "            if currentSegment['type'] == label:\n",
    "                currentSegment['end'] = end\n",
    "            else:\n",
    "                self.speech[currentSegment['type']].append((currentSegment['start'], currentSegment['end']))\n",
    "                currentSegment['type'] = label\n",
    "                currentSegment['start'] = start\n",
    "                currentSegment['end'] = end\n",
    "    \n",
    "    def __getFeatureVector(self, y, sr):\n",
    "        try:\n",
    "            features = FeatureExtraction()\n",
    "            features.load_y_sr(y, sr)\n",
    "            features.melspectrogram()\n",
    "            features.extractmfcc()\n",
    "            features.extractrms()\n",
    "        except ValueError:\n",
    "            logger.warning(\"Error extracting features\")\n",
    "            return None\n",
    "\n",
    "        featureVector = []\n",
    "        for feature in features.mfcc:\n",
    "            featureVector.append(np.mean(feature))\n",
    "            featureVector.append(np.var(feature))\n",
    "\n",
    "        for feature in features.delta_mfcc:\n",
    "            featureVector.append(np.mean(feature))\n",
    "            featureVector.append(np.var(feature))\n",
    "\n",
    "        for feature in features.delta2_mfcc:\n",
    "            featureVector.append(np.mean(feature))\n",
    "            featureVector.append(np.var(feature))\n",
    "\n",
    "        featureVector.append(np.mean(features.rms))\n",
    "        featureVector.append(np.var(features.rms))\n",
    "        \n",
    "        return featureVector\n",
    "    \n",
    "    def saveCorrectedAudio(self):\n",
    "        NORMAL = 0\n",
    "        STUTTER = 1\n",
    "        if not os.path.isdir(self.correctionsDir):\n",
    "            os.makedirs(self.correctionsDir)\n",
    "        outputFilenamePrefix = os.path.join(self.correctionsDir, os.path.splitext(os.path.basename(self.inputFilename))[0])\n",
    "        \n",
    "        normalSpeech = np.ndarray(shape=(1, 0))\n",
    "        (start, end) = self.speech[NORMAL][0]\n",
    "        normalSpeech = np.append(normalSpeech, self.y[int(start):int(end)])\n",
    "        for (start, end) in self.speech[NORMAL][1:]:\n",
    "            # Smoothing\n",
    "            previousSample = normalSpeech[-1]\n",
    "            nextSample = self.y[int(start)]\n",
    "            if nextSample > previousSample:\n",
    "                low, high = previousSample, nextSample\n",
    "            else:\n",
    "                low, high = nextSample, previousSample\n",
    "            \n",
    "            step = (high - low) / self.smoothingSamples\n",
    "            \n",
    "            normalSpeech = np.append(normalSpeech, np.arange(low, high, step))\n",
    "            normalSpeech = np.append(normalSpeech, self.y[int(start):int(end)])\n",
    "\n",
    "        stutteredSpeech = np.ndarray(shape=(1, 0))\n",
    "        for (start, end) in self.speech[STUTTER]:\n",
    "            stutteredSpeech = np.append(stutteredSpeech, self.y[int(start):int(end)])\n",
    "\n",
    "        # Resampling the audio\n",
    "        logger.debug(\"Resampling corrected audio from %d to %d\", self.sr, self.target_sr)\n",
    "        resampledNormalSpeech = librosa.resample(normalSpeech, self.sr, self.target_sr)\n",
    "        resampledStutteredSpeech = librosa.resample(stutteredSpeech, self.sr, self.target_sr)\n",
    "        librosa.output.write_wav(outputFilenamePrefix + \"-corrected.wav\", normalSpeech, self.sr)\n",
    "        librosa.output.write_wav(outputFilenamePrefix + \"-stuttered.wav\", stutteredSpeech, self.sr)\n",
    "        logger.info(\"Corrected audio saved as %s\", outputFilenamePrefix + \"-corrected.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train=False, correct=False):\n",
    "    if train:\n",
    "        dataset = Dataset('dataset', 'datasetLabels.txt', 'datasetArray80.gz')\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(dataset.X, dataset.Y)\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        nn = NeuralNetwork(X_train, Y_train, X_test, Y_test)\n",
    "        nn.train()\n",
    "\n",
    "    if correct:\n",
    "        audiofile = 'M_0219_11y2m_1.wav'\n",
    "        if train:\n",
    "            tfSessionFile = nn.getModelPath()\n",
    "        else:\n",
    "            tfSessionFile = 'tfSessions/2017-11-26-20:08:45-0.870725/session.ckpt'\n",
    "\n",
    "        correction = AudioCorrection(audiofile, tfSessionFile)\n",
    "        correction.process()\n",
    "        correction.saveCorrectedAudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    | Loading file M_0219_11y2m_1.wav\u001b[0m\n",
      "\u001b[32mINFO    | Loading file M_0219_11y2m_1.wav\u001b[0m\n",
      "\u001b[32mINFO    | Loading file M_0219_11y2m_1.wav\u001b[0m\n",
      "I0907 17:46:44.924621 4607980992 <ipython-input-26-bfdef56ad68e>:26] Loading file M_0219_11y2m_1.wav\n",
      "\u001b[32mINFO    | Attempting to correct M_0219_11y2m_1.wav\u001b[0m\n",
      "\u001b[32mINFO    | Attempting to correct M_0219_11y2m_1.wav\u001b[0m\n",
      "\u001b[32mINFO    | Attempting to correct M_0219_11y2m_1.wav\u001b[0m\n",
      "I0907 17:46:53.011835 4607980992 <ipython-input-26-bfdef56ad68e>:33] Attempting to correct M_0219_11y2m_1.wav\n",
      "100% |########################################################################|\n",
      "W0907 17:47:06.735611 4607980992 deprecation.py:323] From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "tfSessions/2017-11-26-20:08:45-0.870725; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-29b99885c678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-ecf9dfd06e1e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(train, correct)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcorrection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioCorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudiofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfSessionFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcorrection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mcorrection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveCorrectedAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-bfdef56ad68e>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mclassificationResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadAndClassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfSessionFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished classification of segments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-f871aed6f033>\u001b[0m in \u001b[0;36mloadAndClassify\u001b[0;34m(self, filename, X)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mprediction_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprediction_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[1;32m   1278\u001b[0m                        compat.as_text(save_path))\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_management.py\u001b[0m in \u001b[0;36mcheckpoint_exists\u001b[0;34m(checkpoint_prefix)\u001b[0m\n\u001b[1;32m    370\u001b[0m   pathname = _prefix_to_checkpoint_path(checkpoint_prefix,\n\u001b[1;32m    371\u001b[0m                                         saver_pb2.SaverDef.V2)\n\u001b[0;32m--> 372\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mfilesystem\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0mlisting\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \"\"\"\n\u001b[0;32m--> 363\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_matching_files_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files_v2\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[0;32m--> 384\u001b[0;31m             compat.as_bytes(pattern))\n\u001b[0m\u001b[1;32m    385\u001b[0m     ]\n\u001b[1;32m    386\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: tfSessions/2017-11-26-20:08:45-0.870725; No such file or directory"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
